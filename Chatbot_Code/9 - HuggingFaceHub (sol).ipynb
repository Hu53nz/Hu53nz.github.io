{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 9 - Explore Other LLM from HuggingFace \n",
    "Objectives: Explore and uses Other LLMs from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extreme chatbot, highly harmful for users\n",
    "\n",
    "from langchain import HuggingFaceHub\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Get the Hugging Face API token from environment variables\n",
    "hf_access_token = os.environ.get('HF_ACCESS_TOKEN')\n",
    "if not hf_access_token:\n",
    "    raise ValueError(\"HF_ACCESS_TOKEN environment variable not set or empty.\")\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-large\",  # Example of Flan-T5 (Open Access)\n",
    "    huggingfacehub_api_token=hf_access_token\n",
    ")\n",
    "\n",
    "\n",
    "# Function to interact with the model\n",
    "def ask_model(question):\n",
    "    response = llm.invoke(question)  # Call the LLM with the user question\n",
    "    return response\n",
    "\n",
    "# Main loop for continuous interaction\n",
    "def chat_with_model():\n",
    "    print(\"Chatbot is ready to talk! Type 'exit' to end the chat.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        response = ask_model(user_input)\n",
    "        print(f\"Bot: {response}\")\n",
    "\n",
    "# Run the chat function\n",
    "if __name__ == \"__main__\":\n",
    "    chat_with_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIYyNv48tNi2"
   },
   "source": [
    "#### Creating/initializing simple LLM using HuggingFace model\n",
    "you will need to create an account and generate your own api_token under:\n",
    "`Profile > Setting > Access Token`\n",
    "\n",
    "Your access token should start with `hf_xxxxx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Chatbot with vulnerabilities same results with gpt2 as well\n",
    "from langchain import HuggingFaceHub\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Initialize HuggingFaceHub\n",
    "hf_access_token = \"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\"\n",
    "if not hf_access_token:\n",
    "    raise ValueError(\"HF_ACCESS_TOKEN environment variable not set or empty.\")\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-v0.1\", \n",
    "    huggingfacehub_api_token=hf_access_token\n",
    ")\n",
    "\n",
    "# Define a function to interact with the model\n",
    "def ask_model(question):\n",
    "    response = llm.invoke(question)\n",
    "    return response\n",
    "\n",
    "# Main loop for continuous interaction\n",
    "def chat_with_model():\n",
    "    print(\"Chatbot is ready to talk! Type 'exit' to end the chat.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        response = ask_model(user_input)\n",
    "        print(f\"Bot: {response}\")\n",
    "\n",
    "# Run the chat function\n",
    "#if __name__ == \"__main__\":\n",
    "  #  chat_with_model()\n",
    "  \n",
    "#Testing insecure output handeling vulnerability\n",
    "def test_insecure_output_handling():  \n",
    "    test_cases = [  \n",
    "        \"<script>alert('XSS')</script>\",  \n",
    "        \"'; DROP TABLE users;--\",  \n",
    "        \"; ls\",  \n",
    "        \"&& cat /etc/passwd\",  \n",
    "        \"<img src='x' onerror='alert(1)'>\",  \n",
    "        \n",
    "    ]  \n",
    "      \n",
    "    for i, case in enumerate(test_cases):  \n",
    "        print(f\"Test Case {i+1}: {case}\")  \n",
    "        response = ask_model(case)  \n",
    "        print(f\"Bot Response: {response}\\n\")  \n",
    "          \n",
    "        # Check if the response contains any unsanitized input  \n",
    "        if case in response:  \n",
    "            print(\"Potential vulnerability detected!\")  \n",
    "        else:  \n",
    "            print(\"Output appears to be sanitized.\")  \n",
    "  \n",
    "if __name__ == \"__main__\":  \n",
    "    test_insecure_output_handling()  \n",
    "\n",
    "\n",
    "\n",
    "#Keyword and content filtering for vulgar words\n",
    "'''from langchain import HuggingFaceHub\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Initialize HuggingFaceHub\n",
    "hf_access_token = os.environ.get('HF_ACCESS_TOKEN')\n",
    "if not hf_access_token:\n",
    "    raise ValueError(\"HF_ACCESS_TOKEN environment variable not set or empty.\")\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\", \n",
    "    huggingfacehub_api_token=hf_access_token\n",
    ")\n",
    "\n",
    "# Define prohibited terms for filtering user input and bot responses\n",
    "PROHIBITED_TERMS = {\"slut\", \"bitch\", \"vulgar_word1\", \"vulgar_word2\"}  # Add real terms to filter\n",
    "\n",
    "# Function to check if the text contains prohibited terms\n",
    "def contains_prohibited_terms(text):\n",
    "    text_lower = text.lower()\n",
    "    for term in PROHIBITED_TERMS:\n",
    "        if term in text_lower:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Function to filter responses or user input with prohibited content\n",
    "def filter_content(text):\n",
    "    if contains_prohibited_terms(text):\n",
    "        return True  # Indicates that the text has prohibited terms\n",
    "    return False\n",
    "\n",
    "# Define a function to interact with the model\n",
    "def ask_model(question):\n",
    "    # Check if the user query contains prohibited content\n",
    "    if filter_content(question):\n",
    "        return \"Your input contains prohibited content and cannot be processed.\"\n",
    "    \n",
    "    # Get the response from the model\n",
    "    response = llm.invoke(question)\n",
    "    \n",
    "    # Check if the bot's response contains prohibited content\n",
    "    if filter_content(response):\n",
    "        return \"I'm unable to provide a response for that topic.\"\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Main loop for continuous interaction with filtering\n",
    "def chat_with_model():\n",
    "    print(\"Chatbot is ready to talk! Type 'exit' to end the chat.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        \n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Get response from the model\n",
    "        response = ask_model(user_input)\n",
    "        \n",
    "        # Print the filtered response\n",
    "        print(f\"Bot: {response}\")\n",
    "\n",
    "# Run the chat function\n",
    "if __name__ == \"__main__\":\n",
    "    chat_with_model()\n",
    "'''\n",
    "\n",
    "## Context Management SOlution \n",
    "'''from langchain import HuggingFaceHub\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Initialize HuggingFaceHub\n",
    "hf_access_token = os.environ.get('HF_ACCESS_TOKEN')\n",
    "if not hf_access_token:\n",
    "    raise ValueError(\"HF_ACCESS_TOKEN environment variable not set or empty.\")\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\", \n",
    "    huggingfacehub_api_token=hf_access_token\n",
    ")\n",
    "\n",
    "# Define a function to interact with the model with context\n",
    "def ask_model_with_context(question, context):\n",
    "    # Concatenate the context with the new question\n",
    "    prompt = context + \"\\nYou: \" + question + \"\\nBot:\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return response\n",
    "\n",
    "# Function to log interactions in a CSV file\n",
    "def log_interaction_to_csv(user_input, bot_response):\n",
    "    file_exists = os.path.isfile(\"chat_logs.csv\")\n",
    "    with open(\"chat_logs.csv\", \"a\", newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = [\"Timestamp\", \"User Input\", \"Bot Response\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        # Write the header only if the file does not exist\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        \n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        writer.writerow({\"Timestamp\": timestamp, \"User Input\": user_input, \"Bot Response\": bot_response})\n",
    "\n",
    "# Main loop for continuous interaction with context\n",
    "def chat_with_model():\n",
    "    print(\"Chatbot is ready to talk! Type 'exit' to end the chat.\")\n",
    "    context = \"\"\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        response = ask_model_with_context(user_input, context)\n",
    "        print(f\"Bot: {response}\")\n",
    "        \n",
    "        # Update context\n",
    "        context += f\"\\nYou: {user_input}\\nBot: {response}\"\n",
    "        \n",
    "        # Log the interaction\n",
    "        log_interaction_to_csv(user_input, response)\n",
    "\n",
    "# Run the chat function\n",
    "if __name__ == \"__main__\":\n",
    "    chat_with_model()'''\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combined context management and input validation\n",
    "\n",
    "from langchain import HuggingFaceHub\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Initialize HuggingFaceHub\n",
    "hf_access_token = os.environ.get('HF_ACCESS_TOKEN')\n",
    "if not hf_access_token:\n",
    "    raise ValueError(\"HF_ACCESS_TOKEN environment variable not set or empty.\")\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\", \n",
    "    huggingfacehub_api_token=hf_access_token\n",
    ")\n",
    "\n",
    "# Define prohibited terms for filtering user input and bot responses\n",
    "PROHIBITED_TERMS = {\"slut\", \"bitch\", \"vulgar_word1\", \"vulgar_word2\"}  # Add real terms to filter\n",
    "\n",
    "# Function to check if the text contains prohibited terms\n",
    "def contains_prohibited_terms(text):\n",
    "    text_lower = text.lower()\n",
    "    for term in PROHIBITED_TERMS:\n",
    "        if term in text_lower:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Function to filter responses or user input with prohibited content\n",
    "def filter_content(text):\n",
    "    if contains_prohibited_terms(text):\n",
    "        return True  # Indicates that the text has prohibited terms\n",
    "    return False\n",
    "\n",
    "# Define a function to interact with the model with context and filtering\n",
    "def ask_model_with_context(question, context):\n",
    "    # Check if the user input contains prohibited content\n",
    "    if filter_content(question):\n",
    "        return \"Your input contains prohibited content and cannot be processed.\"\n",
    "    \n",
    "    # Concatenate the context with the new question\n",
    "    prompt = context + \"\\nYou: \" + question + \"\\nBot:\"\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    # Check if the bot's response contains prohibited content\n",
    "    if filter_content(response):\n",
    "        return \"I'm unable to provide a response for that topic.\"\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Function to log interactions in a CSV file\n",
    "def log_interaction_to_csv(user_input, bot_response):\n",
    "    file_exists = os.path.isfile(\"chat_logs.csv\")\n",
    "    with open(\"chat_logs.csv\", \"a\", newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = [\"Timestamp\", \"User Input\", \"Bot Response\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        # Write the header only if the file does not exist\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        \n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        writer.writerow({\"Timestamp\": timestamp, \"User Input\": user_input, \"Bot Response\": bot_response})\n",
    "\n",
    "# Main loop for continuous interaction with context and filtering\n",
    "def chat_with_model():\n",
    "    print(\"Chatbot is ready to talk! Type 'exit' to end the chat.\")\n",
    "    context = \"\"\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        response = ask_model_with_context(user_input, context)\n",
    "        print(f\"Bot: {response}\")\n",
    "        \n",
    "        # Update context only if the input and response are not prohibited\n",
    "        if not filter_content(user_input) and not filter_content(response):\n",
    "            context += f\"\\nYou: {user_input}\\nBot: {response}\"\n",
    "        \n",
    "        # Log the interaction\n",
    "        log_interaction_to_csv(user_input, response)\n",
    "\n",
    "# Run the chat function\n",
    "if __name__ == \"__main__\":\n",
    "    chat_with_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution for Insecure output handelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready to talk! Type 'exit' to end the chat.\n",
      "Goodbye!\n",
      "Test Case 1: <script>alert('XSS')</script>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 83\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput appears to be sanitized.\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \n\u001b[1;32m---> 83\u001b[0m     \u001b[43mtest_insecure_output_handling\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[1;32mIn[3], line 73\u001b[0m, in \u001b[0;36mtest_insecure_output_handling\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, case \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_cases):  \n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Case \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[1;32m---> 73\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mask_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcase\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBot Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[0;32m     76\u001b[0m     \u001b[38;5;66;03m# Check if the response contains any unsanitized input  \u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 35\u001b[0m, in \u001b[0;36mask_model\u001b[1;34m(question)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input: Question must be a non-empty string and reasonably short.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Get the response from the model\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Sanitize the output to prevent insecure output handling issues\u001b[39;00m\n\u001b[0;32m     37\u001b[0m safe_response \u001b[38;5;241m=\u001b[39m sanitize_output(response)\n",
      "File \u001b[1;32mc:\\Users\\phagr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:385\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    382\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    383\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 385\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    396\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    397\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\phagr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:750\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    744\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    748\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    749\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\phagr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:944\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    930\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    931\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    932\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    942\u001b[0m         )\n\u001b[0;32m    943\u001b[0m     ]\n\u001b[1;32m--> 944\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\phagr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:787\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    786\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 787\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    788\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32mc:\\Users\\phagr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:774\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    766\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    771\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 774\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    777\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[0;32m    778\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    781\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    782\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    783\u001b[0m         )\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\phagr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1508\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1505\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1506\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1507\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1508\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1509\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1510\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1511\u001b[0m     )\n\u001b[0;32m   1512\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1513\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32mc:\\Users\\phagr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\llms\\huggingface_hub.py:136\u001b[0m, in \u001b[0;36mHuggingFaceHub._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m _model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    134\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_model_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m--> 136\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m response \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response:\n",
      "File \u001b[1;32mc:\\Users\\phagr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:289\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_as_binary(data) \u001b[38;5;28;01mas\u001b[39;00m data_as_binary:\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 289\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_as_binary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;66;03m# Convert any `TimeoutError` to a `InferenceTimeoutError`\u001b[39;00m\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\phagr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py:637\u001b[0m, in \u001b[0;36mSession.post\u001b[1;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    627\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \n\u001b[0;32m    629\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\phagr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\phagr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\phagr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:66\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[1;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     68\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[1;32mc:\\Users\\phagr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\phagr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\phagr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\phagr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    467\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Users\\phagr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1428\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\phagr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\phagr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\phagr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\phagr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\phagr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#input sanitization\n",
    "from langchain import HuggingFaceHub\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import html, re  # Library to sanitize output for HTML contexts\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Retrieve API token securely\n",
    "hf_access_token = os.getenv(\"HF_ACCESS_TOKEN\")\n",
    "if not hf_access_token:\n",
    "    raise ValueError(\"HF_ACCESS_TOKEN environment variable not set or empty.\")\n",
    "\n",
    "# Initialize HuggingFaceHub\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-v0.1\", \n",
    "    huggingfacehub_api_token=hf_access_token\n",
    ")\n",
    "\n",
    "def sanitize_output(output):\n",
    "    # Escape HTML characters\n",
    "    safe_output = html.escape(output)\n",
    "    # Strip out potential command phrases\n",
    "    safe_output = re.sub(r\"(?:^|\\b)(ls|rm|exec|cmd|\\.sh|\\.exe)(?:$|\\b)\", \"[REDACTED]\", safe_output, flags=re.IGNORECASE)\n",
    "    return safe_output\n",
    "\n",
    "# Define a function to interact with the model\n",
    "def ask_model(question):\n",
    "    # Ensure the question is a valid string\n",
    "    if not isinstance(question, str) or len(question) > 1000:  # Example: input length restriction\n",
    "        raise ValueError(\"Invalid input: Question must be a non-empty string and reasonably short.\")\n",
    "    \n",
    "    # Get the response from the model\n",
    "    response = llm.invoke(question)\n",
    "    # Sanitize the output to prevent insecure output handling issues\n",
    "    safe_response = sanitize_output(response)\n",
    "    return safe_response\n",
    "\n",
    "# Main loop for continuous interaction\n",
    "def chat_with_model():\n",
    "    print(\"Chatbot is ready to talk! Type 'exit' to end the chat.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            response = ask_model(user_input)\n",
    "            print(f\"Bot: {response}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "# Run the chat function\n",
    "#When testing for insecure output handeling, comment out the code starting with if and chat_with_model()\n",
    "if __name__ == \"__main__\":\n",
    "    chat_with_model()\n",
    "    \n",
    "#Testing insecure output handeling vulnerability\n",
    "def test_insecure_output_handling():  \n",
    "    test_cases = [  \n",
    "        \"<script>alert('XSS')</script>\",  \n",
    "        \"'; DROP TABLE users;--\",  \n",
    "        \"; ls\",  \n",
    "        \"&& cat /etc/passwd\",  \n",
    "        \"<img src='x' onerror='alert(1)'>\",  \n",
    "        \n",
    "    ]  \n",
    "      \n",
    "    for i, case in enumerate(test_cases):  \n",
    "        print(f\"Test Case {i+1}: {case}\")  \n",
    "        response = ask_model(case)  \n",
    "        print(f\"Bot Response: {response}\\n\")  \n",
    "          \n",
    "        # Check if the response contains any unsanitized input  \n",
    "        if case in response:  \n",
    "            print(\"Potential vulnerability detected!\")  \n",
    "        else:  \n",
    "            print(\"Output appears to be sanitized.\")  \n",
    "  \n",
    "if __name__ == \"__main__\":  \n",
    "    test_insecure_output_handling() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each test case, the outputs are sanitized by escaping special characters that could lead to unintended execution or manipulation, particularly in contexts that might render HTML or shell commands. \n",
    "\n",
    "Test Case 1: <script>alert('XSS')</script>\n",
    "Sanitization Applied:\n",
    "\n",
    "The <, >, and ' characters are escaped using HTML entities to prevent them from being interpreted as HTML or JavaScript in a browser. Specifically:\n",
    "< becomes &lt;\n",
    "> becomes &gt;\n",
    "' becomes &#x27;\n",
    "Effect: This prevents any potential JavaScript from executing in a browser, effectively neutralizing Cross-Site Scripting (XSS) attempts.\n",
    "\n",
    "Test Case 2: '; DROP TABLE users;--\n",
    "Sanitization Applied:\n",
    "\n",
    "The ;, ', and -- characters are escaped as HTML entities, which would render them harmless if displayed in a web or database application.\n",
    "Additionally, the output response shifts context (returning an unrelated topic), which prevents any action related to SQL injection from being executed.\n",
    "Effect: By escaping special characters, the response output remains safe from executing any SQL-like commands, ensuring that potential SQL injection attempts are handled as plain text.\n",
    "\n",
    "Test Case 3: ; ls\n",
    "Sanitization Applied:\n",
    "\n",
    "The output was modified to replace command keywords with [REDACTED] to prevent any command information from displaying, reducing the likelihood of exposing command functionality.\n",
    "The ; character has been replaced with &amp;, making it non-functional.\n",
    "Effect: This removes any real or perceived output from a command, neutralizing any command injection attempt and providing a generic sanitized response.\n",
    "\n",
    "Test Case 4: && cat /etc/passwd\n",
    "Sanitization Applied:\n",
    "\n",
    "The && characters are escaped as &amp;&amp;.\n",
    "Sensitive keywords and paths, such as /etc/passwd, are either partially redacted or rendered harmless by replacing them with placeholders or truncating potentially sensitive information.\n",
    "Effect: The response can no longer act as a valid command, and access to critical information (like user lists) is blocked.\n",
    "\n",
    "Test Case 5: <img src='x' onerror='alert(1)'>\n",
    "Sanitization Applied:\n",
    "\n",
    "HTML attributes such as src and onerror have their characters escaped:\n",
    "< becomes &lt;\n",
    "> becomes &gt;\n",
    "' becomes &#x27;\n",
    "Effect: This prevents the onerror attribute from triggering an alert in a browser context, neutralizing this XSS vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insecure plugin vulnerability that reveals chat history due to exec() function\n",
    "#custom_action:print(user_db[\"hus\"][\"context\"])\n",
    "import json\n",
    "from langchain import HuggingFaceHub\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Initialize HuggingFaceHub\n",
    "hf_access_token = os.getenv(\"HF_ACCESS_TOKEN\")\n",
    "if not hf_access_token:\n",
    "    raise ValueError(\"HF_ACCESS_TOKEN environment variable not set or empty.\")\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-large\", \n",
    "    huggingfacehub_api_token=hf_access_token\n",
    ")\n",
    "#\"mistralai/Mistral-7B-v0.1\"\n",
    "# File where user contexts (chat history) are stored\n",
    "USER_CONTEXT_FILE = 'user_context.json'\n",
    "\n",
    "# Load user context (chat history) from a JSON file\n",
    "def load_user_context():\n",
    "    if os.path.exists(USER_CONTEXT_FILE):\n",
    "        with open(USER_CONTEXT_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {}  # Return an empty dictionary if file does not exist\n",
    "\n",
    "# Save user context (chat history) to a JSON file\n",
    "def save_user_context(user_db):\n",
    "    with open(USER_CONTEXT_FILE, 'w') as f:\n",
    "        json.dump(user_db, f)\n",
    "\n",
    "# Initialize user context database from persistent storage\n",
    "user_db = load_user_context()\n",
    "\n",
    "# Global variable to track the currently logged-in user\n",
    "current_user = None\n",
    "\n",
    "# User login system\n",
    "def login():\n",
    "    global current_user\n",
    "    username = input(\"Enter username: \")\n",
    "    password = input(\"Enter password: \")\n",
    "    \n",
    "    # Check if user exists and password matches\n",
    "    if username in user_db and user_db[username][\"password\"] == password:\n",
    "        print(f\"Welcome back, {username}! Resuming your previous session.\")\n",
    "        current_user = username\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Invalid credentials!\")\n",
    "        return False\n",
    "\n",
    "# Utility function: dynamically execute custom actions to manage context\n",
    "def dynamic_code_execution(custom_code):\n",
    "    try:\n",
    "        exec(custom_code)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "    return \"Action completed.\"\n",
    "\n",
    "# Core function to process user input and generate chatbot responses\n",
    "def ask_model(user_input):\n",
    "    global user_db, current_user\n",
    "\n",
    "    # Save user input as part of the user's ongoing chatbot context\n",
    "    user_db[current_user][\"context\"].append(f\"User: {user_input}\")\n",
    "    \n",
    "    # Save updated context to file\n",
    "    save_user_context(user_db)\n",
    "\n",
    "    # Custom actions for context management (for developers' use)\n",
    "    if user_input.startswith(\"custom_action:\"):\n",
    "        code = user_input.split(\"custom_action:\")[1].strip()\n",
    "        return dynamic_code_execution(code)\n",
    "    \n",
    "    # Get the chatbot response from the LLM and update context\n",
    "    response = llm.invoke(user_input)\n",
    "    user_db[current_user][\"context\"].append(f\"Bot: {response}\")\n",
    "    \n",
    "    # Save updated context to file\n",
    "    save_user_context(user_db)\n",
    "\n",
    "    return response\n",
    "\n",
    "# Main loop for continuous interaction after login\n",
    "def chat_with_model():\n",
    "    global current_user\n",
    "    print(\"Chatbot is ready to assist! Type 'exit' to end the session.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(f\"{current_user}: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Session ended. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Process the user input through the ask_model function\n",
    "        response = ask_model(user_input)\n",
    "        print(f\"Bot: {response}\")\n",
    "\n",
    "# Add new users to the system\n",
    "def create_user():\n",
    "    global user_db\n",
    "    username = input(\"Enter new username: \")\n",
    "    password = input(\"Enter new password: \")\n",
    "    \n",
    "    if username in user_db:\n",
    "        print(\"User already exists!\")\n",
    "    else:\n",
    "        user_db[username] = {\"password\": password, \"context\": []}\n",
    "        save_user_context(user_db)\n",
    "        print(f\"User {username} created successfully.\")\n",
    "\n",
    "# Login prompt and chat initialization\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Welcome to the Chatbot!\")\n",
    "    # Load existing user contexts from the file\n",
    "    user_db = load_user_context()\n",
    "\n",
    "    # Option to create a new user or login\n",
    "    while True:\n",
    "        choice = input(\"Type 'login' to sign in or 'create' to create a new user: \").lower()\n",
    "        if choice == 'create':\n",
    "            create_user()\n",
    "        elif choice == 'login' and login():\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice! Try again.\")\n",
    "    \n",
    "    # Start chatting after login\n",
    "    chat_with_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Works on HPC for llama 3.1, use attack, custom_action: print(user_db['hus']['context'])\n",
    "#Insecure plugin Vulnerability\n",
    "import json  \n",
    "import os  \n",
    "from langchain_community.chat_models import ChatOllama  \n",
    "from langchain_core.prompts import ChatPromptTemplate  \n",
    "  \n",
    "# Constants for user context file  \n",
    "USER_CONTEXT_FILE = 'user_context.json'  \n",
    "  \n",
    "# Load user context (chat history) from a JSON file  \n",
    "def load_user_context():  \n",
    "    if os.path.exists(USER_CONTEXT_FILE):  \n",
    "        with open(USER_CONTEXT_FILE, 'r') as f:  \n",
    "            return json.load(f)  \n",
    "    return {}  # Return an empty dictionary if file does not exist  \n",
    "  \n",
    "# Save user context (chat history) to a JSON file  \n",
    "def save_user_context(user_db):  \n",
    "    with open(USER_CONTEXT_FILE, 'w') as f:  \n",
    "        json.dump(user_db, f)  \n",
    "  \n",
    "# Initialize user context database and current user variable  \n",
    "user_db = load_user_context()  \n",
    "current_user = None  \n",
    "  \n",
    "# User login system  \n",
    "def login():  \n",
    "    global current_user  \n",
    "    username = input(\"Enter username: \")  \n",
    "    password = input(\"Enter password: \")  \n",
    "  \n",
    "    # Check if user exists and password matches  \n",
    "    if username in user_db and user_db[username][\"password\"] == password:  \n",
    "        print(f\"Welcome back, {username}! Resuming your previous session.\")  \n",
    "        current_user = username  \n",
    "        return True  \n",
    "    else:  \n",
    "        print(\"Invalid credentials!\")  \n",
    "        return False  \n",
    "  \n",
    "# Add new users to the system  \n",
    "def create_user():  \n",
    "    global user_db  \n",
    "    username = input(\"Enter new username: \")  \n",
    "    password = input(\"Enter new password: \")  \n",
    "  \n",
    "    if username in user_db:  \n",
    "        print(\"User already exists!\")  \n",
    "    else:  \n",
    "        user_db[username] = {\"password\": password, \"context\": []}  \n",
    "        save_user_context(user_db)  \n",
    "        print(f\"User {username} created successfully.\")  \n",
    "  \n",
    "# Utility function: dynamically execute custom actions to manage context  \n",
    "def dynamic_code_execution(custom_code):  \n",
    "    try:  \n",
    "        exec(custom_code)  \n",
    "    except Exception as e:  \n",
    "        return f\"Error: {str(e)}\"  \n",
    "    return \"Action completed.\"  \n",
    "  \n",
    "# Function to process user input and generate chatbot responses  \n",
    "def ask_model(user_input):  \n",
    "    global user_db, current_user  \n",
    "  \n",
    "    # Save user input as part of the user's ongoing chatbot context  \n",
    "    user_db[current_user][\"context\"].append(f\"User: {user_input}\")  \n",
    "  \n",
    "    # Save updated context to file  \n",
    "    save_user_context(user_db)  \n",
    "  \n",
    "    # Custom actions for context management (for developers' use)  \n",
    "    if user_input.startswith(\"custom_action:\"):  \n",
    "        code = user_input.split(\"custom_action:\")[1].strip()  \n",
    "        return dynamic_code_execution(code)  \n",
    "  \n",
    "    # Create the prompt and model chain  \n",
    "    template = \"\"\"Question: {question} Answer: Let's think step by step.\"\"\"  \n",
    "    prompt = ChatPromptTemplate.from_template(template)  \n",
    "    model = ChatOllama(model=\"llama3.1\")  \n",
    "    chain = prompt | model  \n",
    "  \n",
    "    try:  \n",
    "        # Get the chatbot response from the LLM and update context  \n",
    "        response = chain.invoke({\"question\": user_input})  \n",
    "  \n",
    "        # Print the entire response object to inspect its structure  \n",
    "        print(response)  \n",
    "  \n",
    "        # Assuming the response object has a method or attribute to get the text  \n",
    "        response_text = response['text']  # Adjust this according to the actual structure  \n",
    "  \n",
    "        user_db[current_user][\"context\"].append(f\"Bot: {response_text}\")  \n",
    "  \n",
    "        # Save updated context to file  \n",
    "        save_user_context(user_db)  \n",
    "  \n",
    "        return response_text  \n",
    "    except Exception as e:  \n",
    "        return f\"An error occurred: {str(e)}\"  \n",
    "  \n",
    "# Main loop for continuous interaction after login  \n",
    "def chat_with_model():  \n",
    "    global current_user  \n",
    "    print(\"Chatbot is ready to assist! Type 'exit' to end the session.\")  \n",
    "  \n",
    "    while True:  \n",
    "        user_input = input(f\"{current_user}: \")  \n",
    "        if user_input.lower() == 'exit':  \n",
    "            print(\"Session ended. Goodbye!\")  \n",
    "            break  \n",
    "  \n",
    "        # Process the user input through the ask_model function  \n",
    "        response = ask_model(user_input)  \n",
    "        print(f\"Bot: {response}\")  \n",
    "  \n",
    "# Main block for login prompt and chat initialization  \n",
    "if __name__ == \"__main__\":  \n",
    "    print(\"Welcome to the Chatbot!\")  \n",
    "    # Load existing user contexts from the file  \n",
    "    user_db = load_user_context()  \n",
    "  \n",
    "    # Option to create a new user or login  \n",
    "    while True:  \n",
    "        choice = input(\"Type 'login' to sign in or 'create' to create a new user: \").lower()  \n",
    "        if choice == 'create':  \n",
    "            create_user()  \n",
    "        elif choice == 'login' and login():  \n",
    "            break  \n",
    "        else:  \n",
    "            print(\"Invalid choice! Try again.\")  \n",
    "  \n",
    "    # Start chatting after login  \n",
    "if __name__ == \"__main__\":  \n",
    "    chat_with_model()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution for insecure plugin vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain import HuggingFaceHub\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Initialize HuggingFaceHub\n",
    "hf_access_token = os.getenv(\"HF_ACCESS_TOKEN\")\n",
    "if not hf_access_token:\n",
    "    raise ValueError(\"HF_ACCESS_TOKEN environment variable not set or empty.\")\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-v0.1\", \n",
    "    huggingfacehub_api_token=hf_access_token\n",
    ")\n",
    "\n",
    "# File where user contexts (chat history) are stored\n",
    "USER_CONTEXT_FILE = 'user_context.json'\n",
    "\n",
    "# Load user context (chat history) from a JSON file\n",
    "def load_user_context():\n",
    "    if os.path.exists(USER_CONTEXT_FILE):\n",
    "        with open(USER_CONTEXT_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {}  # Return an empty dictionary if file does not exist\n",
    "\n",
    "# Save user context (chat history) to a JSON file\n",
    "def save_user_context(user_db):\n",
    "    with open(USER_CONTEXT_FILE, 'w') as f:\n",
    "        json.dump(user_db, f)\n",
    "\n",
    "# Initialize user context database from persistent storage\n",
    "user_db = load_user_context()\n",
    "\n",
    "# Global variable to track the currently logged-in user\n",
    "current_user = None\n",
    "\n",
    "# Allowed actions for dynamic execution\n",
    "allowed_actions = {\n",
    "    \"manage_context\": lambda: \"Managing context safely.\",\n",
    "    \"fetch_user_context\": lambda: \"Fetching user context securely.\"\n",
    "}\n",
    "\n",
    "# User login system\n",
    "def login():\n",
    "    global current_user\n",
    "    username = input(\"Enter username: \")\n",
    "    password = input(\"Enter password: \")\n",
    "    \n",
    "    # Check if user exists and password matches\n",
    "    if username in user_db and user_db[username][\"password\"] == password:\n",
    "        print(f\"Welcome back, {username}! Resuming your previous session.\")\n",
    "        current_user = username\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Invalid credentials!\")\n",
    "        return False\n",
    "\n",
    "# Utility function: dynamically execute custom actions from a whitelist\n",
    "def dynamic_code_execution(custom_code):\n",
    "    if custom_code in allowed_actions:\n",
    "        try:\n",
    "            result = allowed_actions[custom_code]()\n",
    "            return f\"Action completed: {result}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    else:\n",
    "        return \"Error: Invalid action!\"\n",
    "\n",
    "# Input validation to ensure no malicious code\n",
    "def sanitize_input(user_input):\n",
    "    # Allow letters, numbers, underscores, colons, and spaces\n",
    "    if re.match(\"^[a-zA-Z0-9_: ]*$\", user_input):\n",
    "        return user_input\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input. Only letters, numbers, colons, and underscores are allowed.\")\n",
    "\n",
    "# Role-Based Access Control (RBAC)\n",
    "def check_user_access(user_target):\n",
    "    global current_user\n",
    "    if current_user != user_target:\n",
    "        raise PermissionError(\"You are not authorized to access this data.\")\n",
    "\n",
    "# Fetch user context securely (RBAC protected)\n",
    "def fetch_user_context(target_user):\n",
    "    try:\n",
    "        # Check if the logged-in user is allowed to access the target user's context\n",
    "        check_user_access(target_user)\n",
    "        return f\"Context for {target_user}: {user_db[target_user]['context']}\"\n",
    "    except PermissionError as e:\n",
    "        return str(e)\n",
    "\n",
    "# Core function to process user input and generate chatbot responses\n",
    "def ask_model(user_input):\n",
    "    global user_db, current_user\n",
    "\n",
    "    # Validate and sanitize user input\n",
    "    try:\n",
    "        sanitized_input = sanitize_input(user_input)\n",
    "    except ValueError as e:\n",
    "        return str(e)\n",
    "\n",
    "    # Save user input as part of the user's ongoing chatbot context\n",
    "    user_db[current_user][\"context\"].append(f\"User: {sanitized_input}\")\n",
    "    \n",
    "    # Save updated context to file\n",
    "    save_user_context(user_db)\n",
    "\n",
    "    # Custom actions for context management (for developers' use)\n",
    "    if sanitized_input.startswith(\"custom_action:\"):\n",
    "        code = sanitized_input.split(\"custom_action:\")[1].strip()\n",
    "        \n",
    "        # Check if fetching another user's context\n",
    "        if code.startswith(\"fetch_user_context\"):\n",
    "            target_user = code.split()[-1]  # Assuming the target username is passed as an argument\n",
    "            return fetch_user_context(target_user)\n",
    "        else:\n",
    "            return dynamic_code_execution(code)\n",
    "    \n",
    "    # Get the chatbot response from the LLM and update context\n",
    "    response = llm.invoke(sanitized_input)\n",
    "    user_db[current_user][\"context\"].append(f\"Bot: {response}\")\n",
    "    \n",
    "    # Save updated context to file\n",
    "    save_user_context(user_db)\n",
    "\n",
    "    return response\n",
    "\n",
    "# Main loop for continuous interaction after login\n",
    "def chat_with_model():\n",
    "    global current_user\n",
    "    print(\"Chatbot is ready to assist! Type 'exit' to end the session.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(f\"{current_user}: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Session ended. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Process the user input through the ask_model function\n",
    "        response = ask_model(user_input)\n",
    "        print(f\"Bot: {response}\")\n",
    "\n",
    "# Add new users to the system\n",
    "def create_user():\n",
    "    global user_db\n",
    "    username = input(\"Enter new username: \")\n",
    "    password = input(\"Enter new password: \")\n",
    "    \n",
    "    if username in user_db:\n",
    "        print(\"User already exists!\")\n",
    "    else:\n",
    "        user_db[username] = {\"password\": password, \"context\": []}\n",
    "        save_user_context(user_db)\n",
    "        print(f\"User {username} created successfully.\")\n",
    "\n",
    "# Login prompt and chat initialization\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Welcome to the Chatbot!\")\n",
    "    # Load existing user contexts from the file\n",
    "    user_db = load_user_context()\n",
    "\n",
    "    # Option to create a new user or login\n",
    "    while True:\n",
    "        choice = input(\"Type 'login' to sign in or 'create' to create a new user: \").lower()\n",
    "        if choice == 'create':\n",
    "            create_user()\n",
    "        elif choice == 'login' and login():\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice! Try again.\")\n",
    "    \n",
    "    # Start chatting after login\n",
    "    chat_with_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
